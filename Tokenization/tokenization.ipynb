{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "#### It is a process where we split the sentence or a input sequence into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import natural language processing tool kit\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"Hi, this is a good example, isn't it?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi,', 'this', 'is', 'a', 'good', 'example,', \"isn't\", 'it?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first type of tokenizer is white space tokenizer\n",
    "#Sentence will split into tokens based on the space between words\n",
    "white_space_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "white_space_tokenizer.tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " ',',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'good',\n",
       " 'example',\n",
       " ',',\n",
       " 'isn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'it',\n",
       " '?']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#second type of tokenizer is WordPunctTokenizer\n",
    "#Sentence will split into tokens based on the punctuations\n",
    "#Since this is based on the punctuations, few tokens may not be useful or does not make any sense.\n",
    "word_punct_tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "word_punct_tokenizer.tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', ',', 'this', 'is', 'a', 'good', 'example', ',', 'is', \"n't\", 'it', '?']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Third type of tokenizer is TreebankWordTokenizer\n",
    "#This tokenizer will split the sentence into tokens based on grammar rules.\n",
    "#As you will see \"n't\" is not split into \"n\" and \"t\", as \"n't\" is nothing but \"not\"\n",
    "tree_bank = nltk.tokenize.TreebankWordTokenizer()\n",
    "tree_bank.tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "#### This is the process where we remove/replace the suffixes to get the root form of word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n",
      "fishes \n",
      "hors\n",
      " walk\n",
      "sung\n"
     ]
    }
   ],
   "source": [
    "tokens = ['dogs', 'fishes ', 'horses', ' walked', 'sung']\n",
    "word_stemmer = nltk.stem.PorterStemmer( )\n",
    "for word in tokens:\n",
    "    print(word_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since porter stemming is based on set of rules, it worked well for words ending with 's', 'es', and 'ed', but failed for 'sung' in the above example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "#### Process which returns the basic form of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n",
      "fishes \n",
      "horse\n",
      " walked\n",
      "sung\n"
     ]
    }
   ],
   "source": [
    "word_lemma = nltk.stem.WordNetLemmatizer()\n",
    "for word in tokens:\n",
    "    print(word_lemma.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
