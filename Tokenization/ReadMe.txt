
Tokenization: It is the process of splitting a sentence into tokens(words).
This tokenization.ipynb file shows the implementation of basic and famous tokenization methods:
1) White space tokenizer
   - Splits the sentence into words based on the spaces between them.
2) WordPuncTokenizer
   - Splits the sentence into words based on the punctuation marks
3) treebankwordtokenizer
   - Splits based on the grammar rules.
  
Stemming: Process where we remove/replace the suffixes to get the root form of word. It is based on set of rules.

Lemmatization: Process where we get the basic form of word.
